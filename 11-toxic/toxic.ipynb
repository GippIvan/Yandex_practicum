{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\gipp\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\gipp\\anaconda3\\lib\\site-packages (from lightgbm) (1.3.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\gipp\\anaconda3\\lib\\site-packages (from lightgbm) (0.21.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\gipp\\anaconda3\\lib\\site-packages (from lightgbm) (1.16.5)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\gipp\\anaconda3\\lib\\site-packages (from scikit-learn->lightgbm) (0.13.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0\n",
       "5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7  Your vandalism to the Matt Shirvington article...      0\n",
       "8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9  alignment on this subject and which are contra...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comments.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Открыл данные, посмотрел информацию, наличие пропусков и дубликатов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> \n",
    "### <u>КОММЕНТАРИЙ РЕВЬЮЕРА</u>\n",
    "</font>\n",
    "<font color='green'>\n",
    "ок, принято\n",
    "<br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = WordNetLemmatizer()\n",
    "def lemmatize(text):\n",
    "    lemm_list = nltk.word_tokenize(text)\n",
    "    lemm_text = \" \".join([m.lemmatize(w) for w in lemm_list]) \n",
    "    return lemm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(row):\n",
    "    re_txt = re.sub(r'[^a-zA-Z]', ' ', row)\n",
    "    return \" \".join(re_txt.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Написал 2 функции для лемматизации и отчистки текста от лишних символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "comments['text'] = comments['text'].map(lambda x: lemmatize(clear_text(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применил их к колонке text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.to_csv(\"comments.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изаначально я делал проект через юпитер практикума, но у меня постоянно умирало ядро, тк применение функций занимает какое время я сохранил результат в файл comments.csv что бы при перезапуске начать с этого момента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> \n",
    "### <u>КОММЕНТАРИЙ РЕВЬЮЕРА</u>\n",
    "</font>\n",
    "<font color='green'>\n",
    "понял\n",
    "<br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments1 = pd.read_csv('comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D aww He match this background colour I m seem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man I m really not trying to edit war It s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>More I can t make any real suggestion on impro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Congratulations from me a well use the tool we...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Sorry if the word nonsense wa offensive to you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation Why the edits made under my userna...      0\n",
       "1  D aww He match this background colour I m seem...      0\n",
       "2  Hey man I m really not trying to edit war It s...      0\n",
       "3  More I can t make any real suggestion on impro...      0\n",
       "4  You sir are my hero Any chance you remember wh...      0\n",
       "5  Congratulations from me a well use the tool we...      0\n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7  Your vandalism to the Matt Shirvington article...      0\n",
       "8  Sorry if the word nonsense wa offensive to you...      0\n",
       "9  alignment on this subject and which are contra...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments1['text'] = comments1['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = comments1.drop(['toxic'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = comments1['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_1, features_valid, target_train_1, target_valid = train_test_split(features, target, test_size=0.2, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(features_train_1, target_train_1, test_size=0.2, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### определил фитчи и таргет, разбил датафрейм на 3 выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10358"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(target_train == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91766"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(target_train == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = features_train['text'].values.astype('U')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "count_tf_idf =  TfidfVectorizer(stop_words = stopwords, max_features=6000, min_df=5, max_df=0.7)\n",
    "features_tf_idf = count_tf_idf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### создал куорпус из колонки text, загрузил стоп слова английского языка\n",
    "### эмперическим путем подобрал оптимальные параметры TfidfVectorizer, через fit transform применил к корпусу. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_valid = features_valid['text'].values.astype('U')\n",
    "features_tf_idf_valid = count_tf_idf.transform(corpus_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = features_test['text'].values.astype('U')\n",
    "features_tf_idf_test = count_tf_idf.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### создал корпусы из фитчей тестевой и валдационной выборки применил уже без fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31915, 6000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tf_idf_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25532,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#for i in range(3, 9):\n",
    "#    for j in range(20, 40, 5):\n",
    "#        for l in range(200, 300, 10):\n",
    "#            model_1 = LGBMClassifier(num_leaves=j, max_depth= i,  n_estimators=l, class_weight = 'balanced')\n",
    "#            model_1.fit(features_tf_idf, target_train)\n",
    "#            predicted_1 = model_1.predict(features_tf_idf_valid)\n",
    "#            f1 = f1_score(target_valid, predicted_1)\n",
    "#            print('depth', i, 'leaves', j, 'estimators', l, 'F1=', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Учить модель я решил на LGBM в цикле подобрал оптимальные гиперпараметры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- depth 3 leaves 20 estimators 200 F1= 0.7072079024540824\n",
    "- depth 3 leaves 20 estimators 210 F1= 0.708250115225073\n",
    "- depth 3 leaves 20 estimators 220 F1= 0.7093912511471399\n",
    "- depth 3 leaves 20 estimators 230 F1= 0.7119109348787555\n",
    "- depth 3 leaves 20 estimators 240 F1= 0.7158761628793656\n",
    "- depth 3 leaves 20 estimators 250 F1= 0.7157990314769976\n",
    "- depth 3 leaves 20 estimators 260 F1= 0.7149129447388342\n",
    "- depth 3 leaves 20 estimators 270 F1= 0.7147818209270724\n",
    "- depth 3 leaves 20 estimators 280 F1= 0.7175618587809294\n",
    "- depth 3 leaves 20 estimators 290 F1= 0.7189896256202074\n",
    "- depth 3 leaves 25 estimators 200 F1= 0.7072079024540824\n",
    "- depth 3 leaves 25 estimators 210 F1= 0.708250115225073\n",
    "- depth 3 leaves 25 estimators 220 F1= 0.7093912511471399\n",
    "- depth 3 leaves 25 estimators 230 F1= 0.7119109348787555\n",
    "- depth 3 leaves 25 estimators 240 F1= 0.7158761628793656\n",
    "- depth 3 leaves 25 estimators 250 F1= 0.7157990314769976\n",
    "- depth 3 leaves 25 estimators 260 F1= 0.7149129447388342\n",
    "- depth 3 leaves 25 estimators 270 F1= 0.7147818209270724\n",
    "- depth 3 leaves 25 estimators 280 F1= 0.7175618587809294\n",
    "- depth 3 leaves 25 estimators 290 F1= 0.7189896256202074\n",
    "- depth 3 leaves 30 estimators 200 F1= 0.7072079024540824\n",
    "- depth 3 leaves 30 estimators 210 F1= 0.708250115225073\n",
    "- depth 3 leaves 30 estimators 220 F1= 0.7093912511471399\n",
    "- depth 3 leaves 30 estimators 230 F1= 0.7119109348787555\n",
    "- depth 3 leaves 30 estimators 240 F1= 0.7158761628793656\n",
    "- depth 3 leaves 30 estimators 250 F1= 0.7157990314769976\n",
    "- depth 3 leaves 30 estimators 260 F1= 0.7149129447388342\n",
    "- depth 3 leaves 30 estimators 270 F1= 0.7147818209270724\n",
    "- depth 3 leaves 30 estimators 280 F1= 0.7175618587809294\n",
    "- depth 3 leaves 30 estimators 290 F1= 0.7189896256202074\n",
    "- depth 3 leaves 35 estimators 200 F1= 0.7072079024540824\n",
    "- depth 3 leaves 35 estimators 210 F1= 0.708250115225073\n",
    "- depth 3 leaves 35 estimators 220 F1= 0.7093912511471399\n",
    "- depth 3 leaves 35 estimators 230 F1= 0.7119109348787555\n",
    "- depth 3 leaves 35 estimators 240 F1= 0.7158761628793656\n",
    "- depth 3 leaves 35 estimators 250 F1= 0.7157990314769976\n",
    "- depth 3 leaves 35 estimators 260 F1= 0.7149129447388342\n",
    "- depth 3 leaves 35 estimators 270 F1= 0.7147818209270724\n",
    "- depth 3 leaves 35 estimators 280 F1= 0.7175618587809294\n",
    "- depth 3 leaves 35 estimators 290 F1= 0.7189896256202074\n",
    "- depth 4 leaves 20 estimators 200 F1= 0.7157544602358633\n",
    "- depth 4 leaves 20 estimators 210 F1= 0.7170665459483929\n",
    "- depth 4 leaves 20 estimators 220 F1= 0.718999547988549\n",
    "- depth 4 leaves 20 estimators 230 F1= 0.7198677487225729\n",
    "- depth 4 leaves 20 estimators 240 F1= 0.7195323040023985\n",
    "- depth 4 leaves 20 estimators 250 F1= 0.7206453540484015\n",
    "- depth 4 leaves 20 estimators 260 F1= 0.7225864123957092\n",
    "- depth 4 leaves 20 estimators 270 F1= 0.7202477510691639\n",
    "- depth 4 leaves 20 estimators 280 F1= 0.7218133647335884\n",
    "- depth 4 leaves 20 estimators 290 F1= 0.7218465157306676\n",
    "- depth 4 leaves 25 estimators 200 F1= 0.7157544602358633\n",
    "- depth 4 leaves 25 estimators 210 F1= 0.7170665459483929\n",
    "- depth 4 leaves 25 estimators 220 F1= 0.718999547988549\n",
    "- depth 4 leaves 25 estimators 230 F1= 0.7198677487225729\n",
    "- depth 4 leaves 25 estimators 240 F1= 0.7195323040023985\n",
    "- depth 4 leaves 25 estimators 250 F1= 0.7206453540484015\n",
    "- depth 4 leaves 25 estimators 260 F1= 0.7225864123957092\n",
    "- depth 4 leaves 25 estimators 270 F1= 0.7202477510691639\n",
    "- depth 4 leaves 25 estimators 280 F1= 0.7218133647335884\n",
    "- depth 4 leaves 25 estimators 290 F1= 0.7218465157306676\n",
    "- depth 4 leaves 30 estimators 200 F1= 0.7157544602358633\n",
    "- depth 4 leaves 30 estimators 210 F1= 0.7170665459483929\n",
    "- depth 4 leaves 30 estimators 220 F1= 0.718999547988549\n",
    "- depth 4 leaves 30 estimators 230 F1= 0.7198677487225729\n",
    "- depth 4 leaves 30 estimators 240 F1= 0.7195323040023985\n",
    "- depth 4 leaves 30 estimators 250 F1= 0.7206453540484015\n",
    "- depth 4 leaves 30 estimators 260 F1= 0.7225864123957092\n",
    "- depth 4 leaves 30 estimators 270 F1= 0.7202477510691639\n",
    "- depth 4 leaves 30 estimators 280 F1= 0.7218133647335884\n",
    "- depth 4 leaves 30 estimators 290 F1= 0.7218465157306676\n",
    "- depth 4 leaves 35 estimators 200 F1= 0.7157544602358633\n",
    "- depth 4 leaves 35 estimators 210 F1= 0.7170665459483929\n",
    "- depth 4 leaves 35 estimators 220 F1= 0.718999547988549\n",
    "- depth 4 leaves 35 estimators 230 F1= 0.7198677487225729\n",
    "- depth 4 leaves 35 estimators 240 F1= 0.7195323040023985\n",
    "- depth 4 leaves 35 estimators 250 F1= 0.7206453540484015\n",
    "- depth 4 leaves 35 estimators 260 F1= 0.7225864123957092\n",
    "- depth 4 leaves 35 estimators 270 F1= 0.7202477510691639\n",
    "- depth 4 leaves 35 estimators 280 F1= 0.7218133647335884\n",
    "- depth 4 leaves 35 estimators 290 F1= 0.7218465157306676\n",
    "- depth 5 leaves 20 estimators 200 F1= 0.7235382308845577\n",
    "- depth 5 leaves 20 estimators 210 F1= 0.7228699551569506\n",
    "- depth 5 leaves 20 estimators 220 F1= 0.7218847236627649\n",
    "- depth 5 leaves 20 estimators 230 F1= 0.7227561732958747\n",
    "- depth 5 leaves 20 estimators 240 F1= 0.7252325409715045\n",
    "- depth 5 leaves 20 estimators 250 F1= 0.725715972837319\n",
    "- depth 5 leaves 20 estimators 260 F1= 0.7258301498677638\n",
    "- depth 5 leaves 20 estimators 270 F1= 0.7266060428278087\n",
    "- depth 5 leaves 20 estimators 280 F1= 0.7288732394366196\n",
    "- depth 5 leaves 20 estimators 290 F1= 0.7291880029261155\n",
    "- depth 5 leaves 25 estimators 200 F1= 0.7202620217358939\n",
    "- depth 5 leaves 25 estimators 210 F1= 0.7220570749108203\n",
    "- depth 5 leaves 25 estimators 220 F1= 0.7250260532976031\n",
    "- depth 5 leaves 25 estimators 230 F1= 0.7237560903587776\n",
    "- depth 5 leaves 25 estimators 240 F1= 0.7261184113391407\n",
    "- depth 5 leaves 25 estimators 250 F1= 0.7283441367118445\n",
    "- depth 5 leaves 25 estimators 260 F1= 0.7291421856639249\n",
    "- depth 5 leaves 25 estimators 270 F1= 0.7289774392030471\n",
    "- depth 5 leaves 25 estimators 280 F1= 0.7291880029261155\n",
    "- depth 5 leaves 25 estimators 290 F1= 0.7302795258305285\n",
    "- depth 5 leaves 30 estimators 200 F1= 0.7202620217358939\n",
    "- depth 5 leaves 30 estimators 210 F1= 0.7220570749108203\n",
    "- depth 5 leaves 30 estimators 220 F1= 0.7250260532976031\n",
    "- depth 5 leaves 30 estimators 230 F1= 0.7230905337658508\n",
    "- depth 5 leaves 30 estimators 240 F1= 0.7257446181067533\n",
    "- depth 5 leaves 30 estimators 250 F1= 0.7280766396462784\n",
    "- depth 5 leaves 30 estimators 260 F1= 0.7287413717139081\n",
    "- depth 5 leaves 30 estimators 270 F1= 0.7284185842004984\n",
    "- depth 5 leaves 30 estimators 280 F1= 0.7284710017574691\n",
    "- depth 5 leaves 30 estimators 290 F1= 0.7282576866764275\n",
    "- depth 5 leaves 35 estimators 200 F1= 0.7202620217358939\n",
    "- depth 5 leaves 35 estimators 210 F1= 0.7220570749108203\n",
    "- depth 5 leaves 35 estimators 220 F1= 0.7250260532976031\n",
    "- depth 5 leaves 35 estimators 230 F1= 0.7230905337658508\n",
    "- depth 5 leaves 35 estimators 240 F1= 0.7257446181067533\n",
    "- depth 5 leaves 35 estimators 250 F1= 0.7280766396462784\n",
    "- depth 5 leaves 35 estimators 260 F1= 0.7287413717139081\n",
    "- depth 5 leaves 35 estimators 270 F1= 0.7284185842004984\n",
    "- depth 5 leaves 35 estimators 280 F1= 0.7284710017574691\n",
    "- depth 5 leaves 35 estimators 290 F1= 0.7282576866764275\n",
    "- depth 6 leaves 20 estimators 200 F1= 0.7244490459991126\n",
    "- depth 6 leaves 20 estimators 210 F1= 0.7240312361868277\n",
    "- depth 6 leaves 20 estimators 220 F1= 0.7253521126760563\n",
    "- depth 6 leaves 20 estimators 230 F1= 0.7258608058608058\n",
    "- depth 6 leaves 20 estimators 240 F1= 0.7276988428299399\n",
    "- depth 6 leaves 20 estimators 250 F1= 0.728548457827803\n",
    "- depth 6 leaves 20 estimators 260 F1= 0.7284961966062025\n",
    "- depth 6 leaves 20 estimators 270 F1= 0.7279079924297569\n",
    "- depth 6 leaves 20 estimators 280 F1= 0.7299737838625109\n",
    "- depth 6 leaves 20 estimators 290 F1= 0.7303387120220962\n",
    "- depth 6 leaves 25 estimators 200 F1= 0.7242956188228353\n",
    "- depth 6 leaves 25 estimators 210 F1= 0.7249448123620309\n",
    "- depth 6 leaves 25 estimators 220 F1= 0.7248459958932238\n",
    "- depth 6 leaves 25 estimators 230 F1= 0.7258608058608058\n",
    "- depth 6 leaves 25 estimators 240 F1= 0.7280971186192774\n",
    "- depth 6 leaves 25 estimators 250 F1= 0.7288927841075081\n",
    "- depth 6 leaves 25 estimators 260 F1= 0.72841124217271\n",
    "- depth 6 leaves 25 estimators 270 F1= 0.7257175993041461\n",
    "- depth 6 leaves 25 estimators 280 F1= 0.7287814820206726\n",
    "- depth 6 leaves 25 estimators 290 F1= 0.729387814453977\n",
    "- depth 6 leaves 30 estimators 200 F1= 0.7229200531993497\n",
    "- depth 6 leaves 30 estimators 210 F1= 0.7256323029137702\n",
    "- depth 6 leaves 30 estimators 220 F1= 0.7265510144075271\n",
    "- depth 6 leaves 30 estimators 230 F1= 0.726209618476831\n",
    "- depth 6 leaves 30 estimators 240 F1= 0.7283338196673476\n",
    "- depth 6 leaves 30 estimators 250 F1= 0.7298481308411215\n",
    "- depth 6 leaves 30 estimators 260 F1= 0.7296863603209336\n",
    "- \n",
    "- depth 6 leaves 30 estimators 270 F1= 0.732069970845481\n",
    "- depth 6 leaves 30 estimators 280 F1= 0.7325462760530534\n",
    "- depth 6 leaves 30 estimators 290 F1= 0.7327423339630867\n",
    "- depth 6 leaves 35 estimators 200 F1= 0.7284023668639054\n",
    "- depth 6 leaves 35 estimators 210 F1= 0.7288836385115179\n",
    "- depth 6 leaves 35 estimators 220 F1= 0.7290104396412294\n",
    "- depth 6 leaves 35 estimators 230 F1= 0.73165710924864\n",
    "- depth 6 leaves 35 estimators 240 F1= 0.7304933391889913\n",
    "- depth 6 leaves 35 estimators 250 F1= 0.7314436002337815\n",
    "- depth 6 leaves 35 estimators 260 F1= 0.7301541145681885\n",
    "- depth 6 leaves 35 estimators 270 F1= 0.7290940766550522\n",
    "- depth 6 leaves 35 estimators 280 F1= 0.7334306569343066\n",
    "- depth 6 leaves 35 estimators 290 F1= 0.7353711790393012\n",
    "- depth 7 leaves 20 estimators 200 F1= 0.7278582930756843\n",
    "- depth 7 leaves 20 estimators 210 F1= 0.7301726660813579\n",
    "- depth 7 leaves 20 estimators 220 F1= 0.7303814116615519\n",
    "- depth 7 leaves 20 estimators 230 F1= 0.7260630604570436\n",
    "- depth 7 leaves 20 estimators 240 F1= 0.7302278334058917\n",
    "- depth 7 leaves 20 estimators 250 F1= 0.7325615050651231\n",
    "- depth 7 leaves 20 estimators 260 F1= 0.7338943702843878\n",
    "- depth 7 leaves 20 estimators 270 F1= 0.7323005646445634\n",
    "- depth 7 leaves 20 estimators 280 F1= 0.7301816085327183\n",
    "- depth 7 leaves 20 estimators 290 F1= 0.7316791690709752\n",
    "- depth 7 leaves 25 estimators 200 F1= 0.7237456242707118\n",
    "- depth 7 leaves 25 estimators 210 F1= 0.7286571972470348\n",
    "- depth 7 leaves 25 estimators 220 F1= 0.7293497363796132\n",
    "- depth 7 leaves 25 estimators 230 F1= 0.728575606902603\n",
    "- depth 7 leaves 25 estimators 240 F1= 0.7265613679176931\n",
    "- depth 7 leaves 25 estimators 250 F1= 0.7276934201012292\n",
    "- depth 7 leaves 25 estimators 260 F1= 0.7306240928882438\n",
    "- depth 7 leaves 25 estimators 270 F1= 0.7317002464125236\n",
    "- depth 7 leaves 25 estimators 280 F1= 0.7318630295995356\n",
    "- depth 7 leaves 25 estimators 290 F1= 0.7312762566999854\n",
    "- depth 7 leaves 30 estimators 200 F1= 0.7256533800554825\n",
    "- depth 7 leaves 30 estimators 210 F1= 0.7267144319344935\n",
    "- depth 7 leaves 30 estimators 220 F1= 0.7269804150833089\n",
    "- depth 7 leaves 30 estimators 230 F1= 0.7281227173119065\n",
    "- depth 7 leaves 30 estimators 240 F1= 0.728890185212192\n",
    "- depth 7 leaves 30 estimators 250 F1= 0.7252460914881297\n",
    "- depth 7 leaves 30 estimators 260 F1= 0.7308921825050858\n",
    "- depth 7 leaves 30 estimators 270 F1= 0.7307580598315422\n",
    "- depth 7 leaves 30 estimators 280 F1= 0.7310704960835508\n",
    "- depth 7 leaves 30 estimators 290 F1= 0.7311423193861301\n",
    "- depth 7 leaves 35 estimators 200 F1= 0.7259389671361502\n",
    "- depth 7 leaves 35 estimators 210 F1= 0.7264482153306027\n",
    "- depth 7 leaves 35 estimators 220 F1= 0.7266081871345029\n",
    "- depth 7 leaves 35 estimators 230 F1= 0.7256354393609296\n",
    "- depth 7 leaves 35 estimators 240 F1= 0.7267988996670045\n",
    "- depth 7 leaves 35 estimators 250 F1= 0.7281145999131818\n",
    "- depth 7 leaves 35 estimators 260 F1= 0.7281932590771011\n",
    "- depth 7 leaves 35 estimators 270 F1= 0.7298194945848376\n",
    "- depth 7 leaves 35 estimators 280 F1= 0.7345813379770716\n",
    "- depth 7 leaves 35 estimators 290 F1= 0.7338814366075291\n",
    "- depth 8 leaves 20 estimators 200 F1= 0.7289393278044521\n",
    "- depth 8 leaves 20 estimators 210 F1= 0.7301495137175208\n",
    "- depth 8 leaves 20 estimators 220 F1= 0.7291066282420748\n",
    "- depth 8 leaves 20 estimators 230 F1= 0.7293340987370838\n",
    "- depth 8 leaves 20 estimators 240 F1= 0.728679948401892\n",
    "- depth 8 leaves 20 estimators 250 F1= 0.731090020132298\n",
    "- depth 8 leaves 20 estimators 260 F1= 0.7323741007194244\n",
    "- depth 8 leaves 20 estimators 270 F1= 0.7324712643678162\n",
    "- depth 8 leaves 20 estimators 280 F1= 0.7329602525469938\n",
    "- depth 8 leaves 20 estimators 290 F1= 0.7338212082077772\n",
    "- depth 8 leaves 25 estimators 200 F1= 0.7298716452742123\n",
    "- depth 8 leaves 25 estimators 210 F1= 0.7323041071948733\n",
    "- depth 8 leaves 25 estimators 220 F1= 0.7288722238246322\n",
    "- depth 8 leaves 25 estimators 230 F1= 0.7303695150115473\n",
    "- depth 8 leaves 25 estimators 240 F1= 0.7314467224949466\n",
    "- depth 8 leaves 25 estimators 250 F1= 0.731278801843318\n",
    "- depth 8 leaves 25 estimators 260 F1= 0.7327474427315949\n",
    "- depth 8 leaves 25 estimators 270 F1= 0.7353110599078341\n",
    "- depth 8 leaves 25 estimators 280 F1= 0.7356752087532393\n",
    "- depth 8 leaves 25 estimators 290 F1= 0.7356024701996267\n",
    "- depth 8 leaves 30 estimators 200 F1= 0.7314855550667254\n",
    "- depth 8 leaves 30 estimators 210 F1= 0.7299375997678132\n",
    "- depth 8 leaves 30 estimators 220 F1= 0.7316507107629824\n",
    "- depth 8 leaves 30 estimators 230 F1= 0.7307136665703554\n",
    "- depth 8 leaves 30 estimators 240 F1= 0.7314681280646091\n",
    "- depth 8 leaves 30 estimators 250 F1= 0.7318621087552285\n",
    "- depth 8 leaves 30 estimators 260 F1= 0.7326789838337183\n",
    "- depth 8 leaves 30 estimators 270 F1= 0.733064283655232\n",
    "- depth 8 leaves 30 estimators 280 F1= 0.7340532757379411\n",
    "- depth 8 leaves 30 estimators 290 F1= 0.7331226659006033\n",
    "- depth 8 leaves 35 estimators 200 F1= 0.7276967930029153\n",
    "- depth 8 leaves 35 estimators 210 F1= 0.7310001464343242\n",
    "- depth 8 leaves 35 estimators 220 F1= 0.7330409356725147\n",
    "- depth 8 leaves 35 estimators 230 F1= 0.7320490367775832\n",
    "- depth 8 leaves 35 estimators 240 F1= 0.7310545454545454\n",
    "- depth 8 leaves 35 estimators 250 F1= 0.732598607888631\n",
    "- depth 8 leaves 35 estimators 260 F1= 0.7338546191717349\n",
    "- depth 8 leaves 35 estimators 270 F1= 0.7339688041594454\n",
    "- depth 8 leaves 35 estimators 280 F1= 0.7353026353895163\n",
    "- depth 8 leaves 35 estimators 290 F1= 0.7346526072511917\n",
    "- Wall time: 1h 20min 49s"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 валидационной выборки = 0.8498032697294371\n"
     ]
    }
   ],
   "source": [
    "model_1 = LGBMClassifier(num_leaves=25, max_depth= 8,  n_estimators=270, class_weight = 'balanced')\n",
    "model_1.fit(features_tf_idf, target_train)\n",
    "predicted_1 = model_1.predict(features_tf_idf_valid)\n",
    "f1 = f1_score(target_valid, predicted_1, average='macro')\n",
    "print('F1 валидационной выборки =', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 тестевой выборки = 0.8528105807688071\n"
     ]
    }
   ],
   "source": [
    "predicted_2 = model_1.predict(features_tf_idf_test)\n",
    "f1 = f1_score(target_test, predicted_2, average='macro')\n",
    "print('F1 тестевой выборки =', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### вцелом с задачей я справился, как не кажется, F1 на тестевой выборке получился 0.85. Соответсвенно модель будет неплохо искать токсичные комментарии.\n",
    "\n",
    "### При улучшении F1 я заметил некоторую странность, мы имеем около 10к токсичных таргетов и 100к обычных, по хорошему для улучшения качества модели можно было попробовать увеличить положительные и уменьшить отрицательные. Увеличив положительные в 4 раза и отрицательные уменьшив в 2 я практически не увидел изменений F1 метрики"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
